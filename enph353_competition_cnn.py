# -*- coding: utf-8 -*-
"""ENPH353_Competition_CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1clu6M_T_Q_jcze_f1bOq-R1FjjhDiZl6
"""

##Import data generator libraries
import string
import random
from random import randint
import cv2
import numpy as np
import os
from PIL import Image, ImageFont, ImageDraw
import re
#from google.colab.patches import cv2_imshow

##Import more data generator libraries
from keras.preprocessing.image import ImageDataGenerator
from matplotlib import pyplot as plt
from PIL import Image
import numpy as np
import math

##Put images in folder into array and tagged list
image_dir = '/home/fizzer/ros_ws/src/my_controller/node/clue_images'

image_file_list = os.listdir(image_dir)
imgs = []
filenames = []
for i in range(len(image_file_list)):
  imgs.append(cv2.imread('/home/fizzer/ros_ws/src/my_controller/node/clue_images/' + image_file_list[i]))
  filenames.append(image_file_list[i])
image_array = np.array(imgs)
for i in range(len(image_array)):
  image_array[i] = cv2.cvtColor(image_array[i], cv2.COLOR_RGB2BGR)
image_list = list(zip(imgs, filenames))

##Alters and zips plate images into dataset
NUM_IMAGES_TO_GENERATE = 666

datagen = ImageDataGenerator(rotation_range=0, zoom_range=0.01,
                             brightness_range=[0.7, 1.3])

datagen_iterator = datagen.flow(image_array, batch_size=1, shuffle=False)

subimg = []
label = []
for i in range(NUM_IMAGES_TO_GENERATE):
  print(i)
  value = next(datagen_iterator)
  img = value[0].astype('uint8')
  for j in range(12):
    subimg.append(img[178:228, 18+32*j:51+32*j])
    #print(subimg[-1])
    #cv2_imshow(subimg[12*i+j])
    label_letter = image_list[i % len(image_list)][1][j]
    if re.search("[A-Z]", label_letter):
      hot = ord(label_letter) - 65 # Capital 'A' has ASCII value 65
    else: # it's a space
      hot = 26
    label.append(np.zeros(27, int).tolist())
    label[-1][hot] = 1
    #print(label[-1])
dataset = list(zip(subimg, label))

##Organize dataset into training and validation input and output sets
VALIDATION_SPLIT = 0.3
TEST_SPLIT = 0.15

split_index = math.ceil(len(dataset) * (1-VALIDATION_SPLIT))
test_split_index = math.ceil(len(dataset) * (1-TEST_SPLIT))

np.random.shuffle(dataset)
X_dataset_full, Y_dataset_full = zip(*dataset)
X_dataset_full = list(X_dataset_full)
Y_dataset_full = list(Y_dataset_full)

X_train_dataset = X_dataset_full[:split_index]
Y_train_dataset = Y_dataset_full[:split_index]
X_val_dataset = X_dataset_full[split_index:test_split_index]
Y_val_dataset = Y_dataset_full[split_index:test_split_index]
X_test_dataset = X_dataset_full[test_split_index:]
Y_test_dataset = Y_dataset_full[test_split_index:]
print(np.array(Y_train_dataset).shape)

print("X shape: " + str(len(dataset)))
print("Y shape: " + str(len(dataset)))
print("Total examples: {:d}\nTraining examples: {:d}\n"
      "Validation examples: {:d}".format(len(dataset),
             len(X_train_dataset),
             len(X_val_dataset)))

##Import CNN libraries
from tensorflow.keras import layers
from tensorflow.keras import models
from tensorflow.keras import optimizers

from tensorflow.keras.utils import plot_model
from tensorflow.keras import backend

def reset_weights(model):

  ##Resets the weights of the CNN model

  #< @param model: the model to reset

  for ix, layer in enumerate(model.layers):
      if (hasattr(model.layers[ix], 'kernel_initializer') and
          hasattr(model.layers[ix], 'bias_initializer')):
          weight_initializer = model.layers[ix].kernel_initializer
          bias_initializer = model.layers[ix].bias_initializer

          old_weights, old_biases = model.layers[ix].get_weights()

          model.layers[ix].set_weights([
              weight_initializer(shape=old_weights.shape),
              bias_initializer(shape=len(old_biases))])

##Define the neural network structure
conv_model = models.Sequential()
conv_model.add(layers.Conv2D(32, (3, 3), activation='relu',
                             input_shape=(50, 33, 3)))
conv_model.add(layers.MaxPooling2D((2, 2)))
conv_model.add(layers.Conv2D(64, (3, 3), activation='relu'))
conv_model.add(layers.MaxPooling2D((2, 2)))
conv_model.add(layers.Conv2D(128, (3, 3), activation='relu'))
conv_model.add(layers.MaxPooling2D((2, 2)))
conv_model.add(layers.Flatten())
conv_model.add(layers.Dropout(0.5))
conv_model.add(layers.Dense(512, activation='relu'))
conv_model.add(layers.Dense(27, activation='softmax'))

##Set the learning rate and compile the CNN model
LEARNING_RATE = 5e-5
conv_model.compile(loss='categorical_crossentropy',
                   optimizer=optimizers.RMSprop(learning_rate=LEARNING_RATE),
                   metrics=['acc'])
reset_weights(conv_model)

## Run and display the CNN model fitting
history_conv = conv_model.fit(np.array(X_train_dataset), np.array(Y_train_dataset),
                              validation_data=(np.array(X_val_dataset), np.array(Y_val_dataset)),
                              epochs=120,
                              batch_size=16)
conv_model.save("CNN_new.keras")

##Plot training and validation loss over epochs
plt.plot(history_conv.history['loss'])
plt.plot(history_conv.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train loss', 'val loss'], loc='upper left')
plt.show()

##Plot training and validation accuracy over epochs
plt.plot(history_conv.history['acc'])
plt.plot(history_conv.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy (%)')
plt.xlabel('epoch')
plt.legend(['train accuracy', 'val accuracy'], loc='upper left')
plt.show()

# ##Plot the confusion matrix for the training and validation data
import sklearn.metrics

Y_predict = []
for i in range(len(X_dataset_full)):
  img = X_dataset_full[i]

  img_aug = np.expand_dims(img, axis=0)
  vector = conv_model.predict(img_aug, verbose=None)[0]
  letter = chr(np.argmax(vector)+65)
  print(vector)
  print(letter)
  Y_predict.append(vector)

gnd_truth = []
predictions = []
for i in range(len(Y_dataset_full)):
  gnd_truth.append(Y_dataset_full[i].index(max(Y_dataset_full[i])))
  predictions.append(list(Y_predict[i]).index(max(list(Y_predict[i]))))

C_matrix = sklearn.metrics.confusion_matrix(gnd_truth, predictions)

import seaborn as sn
import pandas as pd
import matplotlib.pyplot as plt

df_cm = pd.DataFrame(C_matrix, index = [i for i in "ABCDEFGHIJKLMNOPQRSTUVWXYZ "],
                  columns = [i for i in "ABCDEFGHIJKLMNOPQRSTUVWXYZ "])
plt.figure(figsize=(20,14))
sn.set(font_scale=1.4) # for label size
sn.heatmap(df_cm, annot=True, annot_kws={"size": 16}) # font size

plt.show()

Y_predict = []
for i in range(len(X_test_dataset)):
  img = X_test_dataset[i]

  img_aug = np.expand_dims(img, axis=0)
  Y_predict.append(conv_model.predict(img_aug, verbose=None)[0])

gnd_truth = []
predictions = []
for i in range(len(Y_test_dataset)):
  gnd_truth.append(Y_test_dataset[i].index(max(Y_test_dataset[i])))
  predictions.append(list(Y_predict[i]).index(max(list(Y_predict[i]))))

C_matrix = sklearn.metrics.confusion_matrix(gnd_truth, predictions)

import seaborn as sn
import pandas as pd
import matplotlib.pyplot as plt

df_cm = pd.DataFrame(C_matrix, index = [i for i in "ABCDEFGHIJKLMNOPQRSTUVWXYZ "],
                  columns = [i for i in "ABCDEFGHIJKLMNOPQRSTUVWXYZ "])
plt.figure(figsize=(20,14))
sn.set(font_scale=1.4) # for label size
sn.heatmap(df_cm, annot=True, annot_kws={"size": 16}) # font size

plt.show()

# from ipywidgets import interact
# import ipywidgets as ipywidgets
# # Display images in the training data set.
# def displayImage(index):
#   img = X_dataset_full[index]

#   img_aug = np.expand_dims(img, axis=0)
#   y_predict = conv_model.predict(img_aug)[0]

#   plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
#   caption = ("                  One-Hot Vector\n"+
#              "GND truth: {:}\nPredicted: {:}".
#              format(Y_dataset_full[index], np.round(y_predict, 3)))
#   plt.text(0.5, 0.5, caption,
#            color='orange', fontsize = 9,
#            horizontalalignment='left', verticalalignment='bottom')


# interact(displayImage,
#         index=ipywidgets.IntSlider(min=0, max=len(dataset),
#                                    step=1, value=10))